{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood of Encrypted Neural Networks\n",
    "\n",
    "This tutorial is optional, and can be skipped without loss of continuity.\n",
    "\n",
    "In this tutorial, we'll take a look at how CrypTen performs inference with an encrypted neural network on encrypted data. We'll see how the data remains encrypted through all the operations, and yet is able to obtain accurate results after the computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "\n",
    "crypten.init() \n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings; \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Keep track of all created temporary files so that we can clean up at the end\n",
    "temp_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Linear Layer\n",
    "We'll start by examining how a single Linear layer works in CrypTen. We'll instantiate a torch Linear layer, convert to CrypTen layer, encrypt it, and step through some toy data with it. As in earlier tutorials, we'll assume Alice has the rank 0 process and Bob has the rank 1 process. We'll also assume Alice has the layer and Bob has the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ALICE and BOB src values\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext Weights:\n",
      "\n",
      " Parameter containing:\n",
      "tensor([[ 0.0616, -0.4290, -0.3774,  0.0635],\n",
      "        [ 0.1681,  0.4551, -0.0549,  0.0142]], requires_grad=True)\n",
      "\n",
      "Plaintext Bias:\n",
      "\n",
      " Parameter containing:\n",
      "tensor([ 0.1682, -0.0193], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate single Linear layer\n",
    "layer_linear = nn.Linear(4, 2)\n",
    "\n",
    "# The weights and the bias are initialized to small random values\n",
    "print(\"Plaintext Weights:\\n\\n\", layer_linear._parameters['weight'])\n",
    "print(\"\\nPlaintext Bias:\\n\\n\", layer_linear._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_linear_file = \"/tmp/tutorial5_layer_alice1.pth\"\n",
    "crypten.save(layer_linear, layer_linear_file)\n",
    "temp_files.append(layer_linear_file) \n",
    "\n",
    "# Generate some toy data\n",
    "features = 4\n",
    "examples = 3\n",
    "toy_data = torch.rand(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob1.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " Parameter encrypted module\n",
      "Bias:\n",
      " Parameter encrypted module \n",
      "\n",
      "Decrypted result:\n",
      " tensor([[-0.1434,  0.2472],\n",
      "        [-0.2202,  0.3145],\n",
      "        [-0.3931,  0.3961]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_single_encrypted_layer():\n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load_from_party(layer_linear_file, src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,4)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Note that layer parameters are encrypted:\n",
    "    crypten.print(\"Weights:\\n\", layer_enc.weight)\n",
    "    crypten.print(\"Bias:\\n\", layer_enc.bias, \"\\n\")\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data_enc = crypten.load_from_party(toy_data_file, src=BOB)\n",
    "    \n",
    "    # Apply the encrypted layer (linear transformation):\n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "    \n",
    "    # Decrypt the result:\n",
    "    result = result_enc.get_plain_text()\n",
    "    \n",
    "    # Examine the result\n",
    "    crypten.print(\"Decrypted result:\\n\", result)\n",
    "        \n",
    "forward_single_encrypted_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the application of the encrypted linear layer on the encrypted data produces an encrypted result, which we can then decrypt to get the values in plaintext.\n",
    "\n",
    "Let's look at a second linear transformation, to give a flavor of how accuracy is preserved even when the data and the layer are encrypted. We'll look at a uniform scaling transformation, in which all tensor elements are multiplied by the same scalar factor. Again, we'll assume Alice has the layer and the rank 0 process, and Bob has the data and the rank 1 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a linear layer with random weights\n",
    "layer_scale = nn.Linear(3, 3)\n",
    "\n",
    "# Construct a uniform scaling matrix: we'll scale by factor 5\n",
    "factor = 5\n",
    "layer_scale._parameters['weight'] = torch.eye(3)*factor\n",
    "layer_scale._parameters['bias'] = torch.zeros_like(layer_scale._parameters['bias'])\n",
    "\n",
    "# Save the plaintext layer\n",
    "layer_scale_file = \"/tmp/tutorial5_layer_alice2.pth\"\n",
    "crypten.save(layer_scale, layer_scale_file)\n",
    "temp_files.append(layer_scale_file)\n",
    "\n",
    "# Construct some toy data\n",
    "features = 3\n",
    "examples = 2\n",
    "toy_data = torch.ones(examples, features)\n",
    "\n",
    "# Save the plaintext toy data\n",
    "toy_data_file = \"/tmp/tutorial5_data_bob2.pth\"\n",
    "crypten.save(toy_data, toy_data_file)\n",
    "temp_files.append(toy_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " Parameter encrypted module\n",
      "Bias:\n",
      "\n",
      " Parameter encrypted module\n",
      "Plaintext result:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def forward_scaling_layer():\n",
    "    rank = comm.get().get_rank()\n",
    "    \n",
    "    # Load and encrypt the layer\n",
    "    layer = crypten.load_from_party(layer_scale_file, src=ALICE)\n",
    "    layer_enc = crypten.nn.from_pytorch(layer, dummy_input=torch.empty((1,3)))\n",
    "    layer_enc.encrypt(src=ALICE)\n",
    "    \n",
    "    # Load and encrypt data\n",
    "    data_enc = crypten.load_from_party(toy_data_file, src=BOB)   \n",
    "    \n",
    "    # Note that layer parameters are (still) encrypted:\n",
    "    crypten.print(\"Weights:\\n\", layer_enc.weight)\n",
    "    crypten.print(\"Bias:\\n\\n\", layer_enc.bias)\n",
    "\n",
    "    # Apply the encrypted scaling transformation\n",
    "    result_enc = layer_enc.forward(data_enc)\n",
    "\n",
    "    # Decrypt the result:\n",
    "    result = result_enc.get_plain_text()\n",
    "    crypten.print(\"Plaintext result:\\n\", (result))\n",
    "        \n",
    "z = forward_scaling_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plaintext tensor is correctly scaled, even though we applied the encrypted transformation on the encrypted input! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Neural Networks\n",
    "Let's now look at how the encrypted input moves through an encrypted multi-layer neural network. \n",
    "\n",
    "For ease of explanation, we'll first step through a network with only two linear layers and ReLU activations. Again, we'll assume Alice has a network and Bob has some data, and they wish to run encrypted inference. \n",
    "\n",
    "To simulate this, we'll once again generate some toy data and train Alice's network on it. Then we'll encrypt Alice's network, Bob's data, and step through every layer in the network with the encrypted data. Through this, we'll see how the computations get applied although the network and the data are encrypted.\n",
    "\n",
    "### Setup\n",
    "As in Tutorial 3, we will first generate 1000 ground truth samples using 50 features and a randomly generated hyperplane to separate positive and negative examples. We will then modify the labels so that they are all non-negative. Finally, we will split the data so that the first 900 samples belong to Alice and the last 100 samples belong to Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "features = 50\n",
    "examples = 1000\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Generate toy data and separating hyperplane\n",
    "data = torch.randn(examples, features)\n",
    "w_true = torch.randn(1, features)\n",
    "b_true = torch.randn(1)\n",
    "labels = w_true.matmul(data.t()).add(b_true).sign()\n",
    "\n",
    "# Change labels to non-negative values\n",
    "labels_nn = torch.where(labels==-1, torch.zeros(labels.size()), labels)\n",
    "labels_nn = labels_nn.squeeze().long()\n",
    "\n",
    "# Split data into Alice's and Bob's portions:\n",
    "data_alice, labels_alice = data[:900], labels_nn[:900]\n",
    "data_bob, labels_bob = data[900:], labels_nn[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Alice's network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AliceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Loss: 0.24704276025295258\n",
      "Epoch 199 Loss: 0.08965432643890381\n",
      "Epoch 299 Loss: 0.051661547273397446\n",
      "Epoch 399 Loss: 0.0351078063249588\n",
      "Epoch 499 Loss: 0.02607247792184353\n"
     ]
    }
   ],
   "source": [
    "# Train and save Alice's network\n",
    "model = AliceNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for i in range(500):  \n",
    "    #forward pass: compute prediction\n",
    "    output = model(data_alice)\n",
    "    \n",
    "    #compute and print loss\n",
    "    loss = criterion(output, labels_alice)\n",
    "    if i % 100 == 99:\n",
    "        print(\"Epoch\", i, \"Loss:\", loss.item())\n",
    "    \n",
    "    #zero gradients for learnable parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #backward pass: compute gradient with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    #update model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "sample_trained_model_file = '/tmp/tutorial5_alice_model.pth'\n",
    "torch.save(model, sample_trained_model_file)\n",
    "temp_files.append(sample_trained_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping through a Multi-layer Network\n",
    "\n",
    "Let's now look at what happens when we load the network Alice's has trained and encrypt it. First, we'll look at how the network structure changes when we convert it from a PyTorch network to CrypTen network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fc1.weight \tModule: Parameter encrypted module\n",
      "Name: fc1.bias \tModule: Parameter encrypted module\n",
      "Name: fc2.weight \tModule: Parameter encrypted module\n",
      "Name: fc2.bias \tModule: Parameter encrypted module\n",
      "Name: /fc1/Gemm_output_0 \tModule: Gemm encrypted module\n",
      "Name: /Relu_output_0 \tModule: ReLU encrypted module\n",
      "Name: output \tModule: Gemm encrypted module\n"
     ]
    }
   ],
   "source": [
    "# Load the trained network to Alice\n",
    "model_plaintext = crypten.load_from_party(sample_trained_model_file, model_class=AliceNet, src=ALICE)\n",
    "\n",
    "# Convert the trained network to CrypTen network \n",
    "private_model = crypten.nn.from_pytorch(model_plaintext, dummy_input=torch.empty((1, 50)))\n",
    "# Encrypt the network\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted CrypTen network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the encrypted network has 3 modules, named '5', '6' and 'output', denoting the first Linear layer, the ReLU activation, and the second Linear layer respectively. These modules are encrypted just as the layers in the previous section were. \n",
    "\n",
    "Now let's encrypt Bob's data, and step it through each encrypted module. For readability, we will use only 3 examples from Bob's data to illustrate the inference. Note how Bob's data remains encrypted after each individual layer's computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Select only the first three examples in Bob's data for readability\n",
    "data = data_bob[:3]\n",
    "sample_data_bob_file = '/tmp/tutorial5_data_bob3.pth'\n",
    "torch.save(data, sample_data_bob_file)\n",
    "temp_files.append(sample_data_bob_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/amithabh/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10976/527380923.py\", line 16, in step_through_two_layers\n",
      "    out_enc = private_model._modules['out'].forward([data_enc, fc1_weight, fc1_bias])\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyError: 'out'\n",
      "  File \"/home/amithabh/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10976/527380923.py\", line 16, in step_through_two_layers\n",
      "    out_enc = private_model._modules['out'].forward([data_enc, fc1_weight, fc1_bias])\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n",
      "KeyError: 'out'\n",
      "ERROR:root:One of the parties failed. Check past logs\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def step_through_two_layers():    \n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    # Load and encrypt the network\n",
    "    model = crypten.load_from_party(sample_trained_model_file, model_class=AliceNet, src=ALICE)\n",
    "    private_model = crypten.nn.from_pytorch(model, dummy_input=torch.empty((1, 50)))\n",
    "    private_model.encrypt(src=ALICE)\n",
    "\n",
    "    # Load and encrypt the data\n",
    "    data_enc = crypten.load_from_party(sample_data_bob_file, src=BOB)\n",
    "\n",
    "    # Forward through the first layer\n",
    "    fc1_weight = private_model._modules['fc1.weight'].forward([])\n",
    "    fc1_bias = private_model._modules['fc1.bias'].forward([])\n",
    "    out_enc = private_model._modules['out'].forward([data_enc, fc1_weight, fc1_bias])\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tFirst Linear Layer: Output Encrypted: {encrypted}\", in_order=True)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tShares after First Linear Layer:{out_enc.share}\", in_order=True)\n",
    "\n",
    "    # Apply ReLU activation\n",
    "    out_enc = private_model._modules['onnx::Gemm_6'].forward(out_enc)\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tReLU:\\n Output Encrypted: {encrypted}\", in_order=True)\n",
    "    crypten.print(f\"Rank: {rank}\\n\\tShares after ReLU: {out_enc.share}\\n\", in_order=True)\n",
    "\n",
    "    # Forward through the second Linear layer\n",
    "    fc2_weight = private_model._modules['fc2.weight'].forward([])\n",
    "    fc2_bias = private_model._modules['fc2.bias'].forward([])\n",
    "    out_enc = private_model._modules['output'].forward([out_enc, fc2_weight, fc2_bias])\n",
    "    encrypted = crypten.is_encrypted_tensor(out_enc)\n",
    "    crypten.print(f\"Rank: {rank} Second Linear layer:\\n Output Encrypted: {encrypted}\\n\", in_order=True) \n",
    "    crypten.print(f\"Rank: {rank} Shares after Second Linear layer:{out_enc.share}\\n\", in_order=True)\n",
    "\n",
    "    # Decrypt the output\n",
    "    out_dec = out_enc.get_plain_text()\n",
    "    \n",
    "    # Since both parties have same decrypted results, only print the rank 0 output\n",
    "    crypten.print(\"Decrypted output:\\n Output Encrypted:\", crypten.is_encrypted_tensor(out_dec))\n",
    "    crypten.print(\"Tensors:\\n\", out_dec)\n",
    "    \n",
    "z = step_through_two_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we emphasize that the output of each layer is an encrypted tensor. Only after the final call to `get_plain_text` do we get the plaintext tensor.\n",
    "\n",
    "### From PyTorch to CrypTen: Structural Changes in Network Architecture \n",
    "\n",
    "We have used a simple two-layer network in the above example, but the same ideas apply to more complex networks and operations. However, in more complex networks, there may not always be a one-to-one mapping between the PyTorch layers and the CrypTen layers. This is because we use PyTorch's onnx implementation to convert PyTorch models to CrypTen models. \n",
    "As an example, we'll take a typical network used to classify digits in MNIST data, and look at what happens to its structure we convert it to a CrypTen module. (As we only wish to illustrate the structural changes in layers, we will not train this network on data; we will just use it with its randomly initialized weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "CrypTen does not support ONNX op Identity.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Encrypt the network\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m private_model \u001b[38;5;241m=\u001b[39m \u001b[43mcrypten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m private_model\u001b[38;5;241m.\u001b[39mencrypt(src\u001b[38;5;241m=\u001b[39mALICE)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Examine the structure of the encrypted network\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/nn/onnx_converter.py:58\u001b[0m, in \u001b[0;36mfrom_pytorch\u001b[0;34m(pytorch_model, dummy_input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# construct CrypTen model:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m f \u001b[38;5;241m=\u001b[39m _from_pytorch_to_bytes(pytorch_model, dummy_input)\n\u001b[0;32m---> 58\u001b[0m crypten_model \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# set model architecture to export model back to pytorch model\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/nn/onnx_converter.py:47\u001b[0m, in \u001b[0;36mfrom_onnx\u001b[0;34m(onnx_string_or_file)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mConverts an ONNX model serialized in an `onnx_string_or_file` to a CrypTen model.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m _load_onnx_model(onnx_string_or_file)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_crypten\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/nn/onnx_converter.py:184\u001b[0m, in \u001b[0;36m_to_crypten\u001b[0;34m(onnx_model)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m onnx_model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnode:\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# get attributes and node type:\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m {attr\u001b[38;5;241m.\u001b[39mname: _get_attribute_value(attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattribute}\n\u001b[0;32m--> 184\u001b[0m     crypten_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_operator_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# add CrypTen module to graph:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     crypten_module \u001b[38;5;241m=\u001b[39m crypten_class\u001b[38;5;241m.\u001b[39mfrom_onnx(attributes\u001b[38;5;241m=\u001b[39mattributes)\n",
      "File \u001b[0;32m~/dev/internship/libraries/CrypTen/venv/lib/python3.12/site-packages/crypten/nn/onnx_converter.py:260\u001b[0m, in \u001b[0;36m_get_operator_class\u001b[0;34m(node_op_type, attributes)\u001b[0m\n\u001b[1;32m    256\u001b[0m crypten_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    257\u001b[0m     module, node_op_type, ONNX_TO_CRYPTEN\u001b[38;5;241m.\u001b[39mget(node_op_type, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crypten_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrypTen does not support ONNX op \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_op_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m crypten_class\n",
      "\u001b[0;31mValueError\u001b[0m: CrypTen does not support ONNX op Identity."
     ]
    }
   ],
   "source": [
    "# Define Alice's network\n",
    "class AliceNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AliceNet2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(100)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.batchnorm3(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "model = AliceNet2()\n",
    "\n",
    "# Let's encrypt the complex network. \n",
    "# Create dummy input of the correct input shape for the model\n",
    "dummy_input = torch.empty((1, 1, 28, 28))\n",
    "\n",
    "# Encrypt the network\n",
    "private_model = crypten.nn.from_pytorch(model, dummy_input)\n",
    "private_model.encrypt(src=ALICE)\n",
    "\n",
    "# Examine the structure of the encrypted network\n",
    "for name, curr_module in private_model._modules.items():\n",
    "    print(\"Name:\", name, \"\\tModule:\", curr_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the CrypTen network has split some the layers in the PyTorch module into several CrypTen modules. Each PyTorch operation may correspond to one or more operations in CrypTen. However, during the conversion, these are sometimes split due to limitations intorduced by onnx.\n",
    "\n",
    "Before exiting this tutorial, please clean up the files generated using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for fn in temp_files:\n",
    "    if os.path.exists(fn): os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
